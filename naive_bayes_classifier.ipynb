{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f8fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nl\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import joblib\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import FreqDist\n",
    "from math import floor\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "230bf682",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    positive_messages= []\n",
    "    negative_messages = []\n",
    "    \n",
    "    dataset = pd.read_csv(\"dataset.csv\")\n",
    "    \n",
    "#     print(dataset.shape)\n",
    "#     print(dataset.isna().sum())\n",
    "    \n",
    "#     dataset = dataset.dropna()\n",
    "\n",
    "#     print(dataset.shape)\n",
    "    \n",
    "    messages = dataset['messages']\n",
    "    label = dataset['label']\n",
    "#     dataset.to_csv(\"editied_sentiment_tweets3.csv\")\n",
    "    length = dataset.shape\n",
    "    \n",
    "#     print(dataset.isna().sum())\n",
    "#     print(\"length:\",length)\n",
    "    for i in range(length[0]):\n",
    "        if label[i] == 1:\n",
    "            positive_messages.append(messages[i])\n",
    "        elif label[i] == -1:\n",
    "            negative_messages.append(messages[i])\n",
    "\n",
    "# for i in range(len(label)):\n",
    "    \n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94a0d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(input_data):\n",
    "    \n",
    "    url_pattern = r'http\\S+'\n",
    "    if re.search(url_pattern,input_data):\n",
    "        input_data = re.sub(url_pattern,'',input_data)\n",
    "            \n",
    "    return input_data\n",
    "\n",
    "def remove_username(input_data):\n",
    "    \n",
    "    username_pattern = '@[^\\s]+'\n",
    "    if re.search(username_pattern,input_data):\n",
    "        input_data = re.sub(username_pattern,'',input_data)\n",
    "            \n",
    "    return input_data\n",
    "\n",
    "def tokenize(input_data):\n",
    "    return nl.word_tokenize(input_data)\n",
    "\n",
    "def remove_character(input_data):\n",
    "    if input_data not in string.punctuation:\n",
    "        return input_data\n",
    "\n",
    "def remove_stopwords(input_data):\n",
    "    stop_words = set(nl.corpus.stopwords.words(\"English\"))\n",
    "    if input_data not in stop_words:\n",
    "        return input_data\n",
    "\n",
    "def stemming(input_data):\n",
    "    ps = PorterStemmer()\n",
    "    store = ps.stem(input_data)\n",
    "    return store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2cae94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(input_file,mode):\n",
    "    \n",
    "    filtered_list = []\n",
    "    tokenized_list = []\n",
    "    stemmed_list = []\n",
    "    \n",
    "    if mode == 'list':\n",
    "        for sentences in input_file:\n",
    "            tokenized_list.append(tokenize(remove_username(remove_url(sentences))))\n",
    "        for sentences in tokenized_list:\n",
    "            for words in sentences:\n",
    "                filtered_list.append(remove_stopwords(remove_character(words)))\n",
    "                \n",
    "        filtered_list = list(filter(None,filtered_list))\n",
    "        for words in filtered_list:\n",
    "            stemmed_list.append(stemming(words))\n",
    "    else:\n",
    "        tokenized = tokenize(remove_username(remove_url(input_file)))     \n",
    "        for sentences in tokenized:\n",
    "#             for words in sentences:\n",
    "            filtered_list.append(remove_stopwords(remove_character(sentences)))\n",
    "            filtered_list = list(filter(None,filtered_list))\n",
    "        for words in filtered_list:\n",
    "            stemmed_list.append(stemming(words))\n",
    "            \n",
    "    return stemmed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f3ea6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(input_list,label,mode):\n",
    "    list_ = []\n",
    "    dictionary = {}\n",
    "    \n",
    "    if mode == 'train':\n",
    "        for twts in input_list:\n",
    "            store = [{twts:True},label]\n",
    "            list_.append(store)\n",
    "        return list_\n",
    "    else:\n",
    "        for twts in input_list:\n",
    "            dictionary[twts] = True\n",
    "        return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73aba786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_distribution(total_data,train_total):\n",
    "\n",
    "    train = floor(total_data*train_total)\n",
    "    test = total_data-train\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e40417e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "noise_free_positive = remove_noise(positive_messages,'list')\n",
    "noise_free_negative = remove_noise(negative_messages,'list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dcbae73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "positive_dictionary = create_dictionary(noise_free_positive,'positive','train')\n",
    "negative_dictionary = create_dictionary(noise_free_negative,'negative','train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd55b69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pre_proessed_negative_messages']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joblib.dump(noise_free_positive,'pre_proessed_positive_messages')\n",
    "# joblib.dump(noise_free_negative,'pre_proessed_negative_messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d596aa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = positive_dictionary + negative_dictionary\n",
    "random.shuffle(total_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e12d57b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_amount = dataset_distribution(len(total_dataset),0.8)\n",
    "train = total_dataset[:train_amount]\n",
    "test = total_dataset[train_amount:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "992a53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveBayesClassifier.train(train)\n",
    "#model = joblib.load(\"sentimental_analysis_model.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18944ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_model = model.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8f77c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.6830403698754526\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy is:\", classify.accuracy(model, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bf8b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(new_model,\"sentimental_analysis_model.sav\")\n",
    "\n",
    "# model1 = joblib.load(\"sentimental_analysis_model.sav\")\n",
    "\n",
    "# msg = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "# store = remove_noise(msg,'sentence')\n",
    "# store1 = create_dictionary(store,'','sentence')\n",
    "# # temp_dictionary = {}\n",
    "# # for words in store:\n",
    "# #     temp_dictionary[words] = True\n",
    "\n",
    "\n",
    "# store1\n",
    "\n",
    "# model1.classify(store1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "69118c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = \"\"\n",
    "msg = remove_noise(msg,'sentence')\n",
    "\n",
    "msg = create_dictionary(msg,'','sentence')\n",
    "\n",
    "model.classify(msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
